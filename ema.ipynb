{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.optim import SGD\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from torchvision import models, transforms\n",
    "from torchvision.datasets import CIFAR10\n",
    "\n",
    "from pytorch_lightning import LightningModule, LightningDataModule, Trainer\n",
    "from torchmetrics import Accuracy, MetricCollection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Define Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CIFAR(LightningDataModule):\n",
    "    def __init__(self, img_size=32, batch_size=32):\n",
    "        super().__init__()\n",
    "        self.img_size = img_size if isinstance(img_size, tuple) else (img_size, img_size)\n",
    "        self.batch_size = batch_size\n",
    "        self.train_transforms = transforms.Compose([\n",
    "            transforms.Resize(self.img_size),\n",
    "            transforms.Pad(4, padding_mode='reflect'),\n",
    "            transforms.RandomCrop(self.img_size),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))\n",
    "        ])\n",
    "\n",
    "        self.test_transforms = transforms.Compose([\n",
    "            transforms.Resize(self.img_size),\n",
    "            transforms.CenterCrop(self.img_size),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))\n",
    "        ])\n",
    "\n",
    "    def prepare_data(self) -> None:\n",
    "        CIFAR10(root='data', train=True, download=True)\n",
    "        CIFAR10(root='data', train=False, download=True)\n",
    "    \n",
    "    def setup(self, stage=None):\n",
    "        self.train_ds = CIFAR10(root='data', train=True, download=False, transform=self.train_transforms)\n",
    "        self.valid_ds = CIFAR10(root='data', train=False, download=False, transform=self.test_transforms)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_ds, num_workers=4, batch_size=self.batch_size, shuffle=True)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.valid_ds, num_workers=4, batch_size=self.batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Define Model & EMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EMA(nn.Module):\n",
    "    \"\"\" Model Exponential Moving Average V2 from timm\"\"\"\n",
    "    def __init__(self, model, decay=0.9999):\n",
    "        super(EMA, self).__init__()\n",
    "        # make a copy of the model for accumulating moving average of weights\n",
    "        self.module = deepcopy(model)\n",
    "        self.module.eval()\n",
    "        self.decay = decay\n",
    "\n",
    "    def _update(self, model, update_fn):\n",
    "        with torch.no_grad():\n",
    "            for ema_v, model_v in zip(self.module.state_dict().values(), model.state_dict().values()):\n",
    "                ema_v.copy_(update_fn(ema_v, model_v))\n",
    "\n",
    "    def update(self, model):\n",
    "        self._update(model, update_fn=lambda e, m: self.decay * e + (1. - self.decay) * m)\n",
    "\n",
    "    def set(self, model):\n",
    "        self._update(model, update_fn=lambda e, m: m)\n",
    "    \n",
    "\n",
    "class BasicModule(LightningModule):\n",
    "    def __init__(self, lr=0.01, use_ema=False):\n",
    "        super().__init__()\n",
    "        self.model = models.resnet18(pretrained=False)\n",
    "        self.model_ema = EMA(self.model, decay=0.9) if use_ema else None\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        self.lr = lr\n",
    "        \n",
    "        metric = MetricCollection({'top@1': Accuracy(top_k=1), 'top@5': Accuracy(top_k=5)})\n",
    "        self.train_metric = metric.clone(prefix='train_')\n",
    "        self.valid_metric = metric.clone(prefix='valid_')\n",
    "    \n",
    "    def training_step(self, batch, batch_idx, optimizer_idx=None):\n",
    "        return self.shared_step(*batch, self.train_metric)\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        return self.shared_step(*batch, self.valid_metric)\n",
    "\n",
    "    def shared_step(self, x, y, metric):\n",
    "        y_hat = self.model(x) if self.training or self.model_ema is None else self.model_ema.module(x)\n",
    "        loss = self.criterion(y_hat, y)\n",
    "        self.log_dict(metric(y_hat, y), prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return SGD(self.model.parameters(), lr=self.lr)\n",
    "\n",
    "    def on_before_backward(self, loss: torch.Tensor) -> None:\n",
    "        if self.model_ema:\n",
    "            self.model_ema.update(self.model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Train without EMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit native Automatic Mixed Precision (AMP)\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7,8]\n",
      "\n",
      "  | Name         | Type             | Params\n",
      "--------------------------------------------------\n",
      "0 | model        | ResNet           | 11.7 M\n",
      "1 | criterion    | CrossEntropyLoss | 0     \n",
      "2 | train_metric | MetricCollection | 0     \n",
      "3 | valid_metric | MetricCollection | 0     \n",
      "--------------------------------------------------\n",
      "11.7 M    Trainable params\n",
      "0         Non-trainable params\n",
      "11.7 M    Total params\n",
      "23.379    Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff38cacb31e947c6ad7face7400de94b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation sanity check: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "051697d9d7ae4935824a834c791fbe40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fdb3bca2ceaf44f19c4920e1b72831ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "680e4d7e4dcf4ae4911c85ee48bb7fb7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = CIFAR(batch_size=512)\n",
    "model = BasicModule(lr=0.01, use_ema=False)\n",
    "trainer = Trainer(max_epochs=2, gpus='5,', precision=16)\n",
    "trainer.fit(model, data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Train with EMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit native Automatic Mixed Precision (AMP)\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7,8]\n",
      "\n",
      "  | Name         | Type             | Params\n",
      "--------------------------------------------------\n",
      "0 | model        | ResNet           | 11.7 M\n",
      "1 | model_ema    | EMA              | 11.7 M\n",
      "2 | criterion    | CrossEntropyLoss | 0     \n",
      "3 | train_metric | MetricCollection | 0     \n",
      "4 | valid_metric | MetricCollection | 0     \n",
      "--------------------------------------------------\n",
      "23.4 M    Trainable params\n",
      "0         Non-trainable params\n",
      "23.4 M    Total params\n",
      "46.758    Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e2a05c5d9164dbe80c3517ede1f8600",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation sanity check: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "393f064ecf0c4e1a8297afbba0e67332",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d196f7a490547f9a3ea2b81b5ba4c5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9de9459fc6e472b9c5870f0b9bc3021",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = CIFAR(batch_size=512)\n",
    "model = BasicModule(lr=0.01, use_ema=True)\n",
    "trainer = Trainer(max_epochs=2, gpus='5,', precision=16)\n",
    "trainer.fit(model, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
